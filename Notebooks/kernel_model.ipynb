{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jshogland/SpatialModelingTutorials/blob/main/Notebooks/kernel_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcZNhvYmw_7v"
   },
   "source": [
    "# Using a neighborhood windows and machine learning to create estimate tree canopy cover\n",
    "## In this notebook we will explore the use of Raster Tools and Scikit learn functions to incorporate texture and hue into the modeling purposes. Key objectives:\n",
    "- Determine and use cell values of a moving window (user defined size) that can be used to optimally transform a given image and its bands into estimates of some known phenomena.\n",
    "- Efficiently create surfaces that account for band and spatial covariation.\n",
    "- Create surfaces that estimate a given phenomena within a raster.\n",
    "### The approach\n",
    "- Use sampling to select train locations\n",
    "- Extract and flatten cell values for a defined area around each location\n",
    "- Train a machine learning model (Gradient Boosted Regression)\n",
    "- Apply the model back to the input data\n",
    "\n",
    "John Hogland 12/30/2024\n",
    "\n",
    "#### Study area for this example includes portions of the Custer Gallatin Nation Forest\n",
    "\n",
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4_t0-07xMKD"
   },
   "outputs": [],
   "source": [
    "!pip install mapclassify\n",
    "!pip install osmnx\n",
    "!pip install raster_tools\n",
    "!pip install planetary-computer\n",
    "!pip install pystac-client\n",
    "!pip install stackstac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5s6NvQ3xbTe"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uuRURE7w_7w"
   },
   "outputs": [],
   "source": [
    "import numpy as np, os, geopandas as gpd, pandas as pd, osmnx as ox, pystac_client, planetary_computer, stackstac\n",
    "from raster_tools import Raster, general\n",
    "from raster_tools import raster\n",
    "from owslib.wcs import WebCoverageService\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Both hue and texture play import roles as predictor values in regression, classification, and clustering projects that aim to estimating some known phenomena using remotely sensed data. While hue is well defined for a given image (i.e., the cell value), texture can be somewhat ambiguous to quantify and is often subjectively determined prior to the modeling process.\n",
    "### In this example we explore using image cell values extracted for a defined neighborhood (window) within the modeling process to produce estimates of percent canopy cover for a portion of the Custer Gallatin National Forest located in southeastern Montana, USA (Figure 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzrUU7gnw_7x"
   },
   "source": [
    "#### Get the boundary data for portions of the Custer Gallatin National Forest and create a interactive location map of the study (Figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdV_EZQgw_7x"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "#use OpenStreetMaps to get the boundary of the NF\n",
    "nf=ox.geocode_to_gdf('Custer Gallatin National Forest, MT, USA')\n",
    "\n",
    "#get first polygon of the NF\n",
    "nfe=nf.explode()\n",
    "nf1=gpd.GeoSeries(nfe.geometry.iloc[10],crs=nf.crs)\n",
    "\n",
    "#project to Albers equal area\n",
    "nf1p=nf1.to_crs(5070)\n",
    "\n",
    "#Visualize the nf1 and sample locations\n",
    "m=nf1p.explore(color='red',style_kwds=dict(fill=False,weight=5))\n",
    "folium.TileLayer(\n",
    "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "    attr=\"Esri\",\n",
    "    name=\"Esri Imagery\",\n",
    "    overlay=False,\n",
    "    control=True,\n",
    ").add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure 1.__ Interactive location map of the study area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### To estimate percent canopy cover using Landsat 8 imagery we implemented a multistep approach. \n",
    "1. Dowload MRLC's 2016 tree canopy cover (TCC) raster surface and part of a Landsat scene for the area around the Custer Gallatin Nation Forest (Figure 1) from MLRC's WCS service and Planetary Computer, respectively. \n",
    "2. Create a simple random sample of 150 locations and extract the cell values (and neighboring window cell values) at those location for the TCC and Landsat datasets.\n",
    "4. Created a Gradient Boosted Regression model (GBRM) using the sample. \n",
    "5. Apply our newly developed GBRM back to our predictor variables. \n",
    "\n",
    "To perform these steps, we created a series of python functions that utilize Scikit Learn and Raster Tools application programming interface (API). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU3LLnKsw_7x"
   },
   "source": [
    "### Step 1. Get Tree Canopy Cover and Landsat 8 Imagery\n",
    "Create download definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2FvPl1L4w_7x"
   },
   "outputs": [],
   "source": [
    "#create definition to mosaic stac data\n",
    "def mosaic_stac(xr):\n",
    "    return stackstac.mosaic(xr)\n",
    "\n",
    "#create definition to extract stac data\n",
    "def get_stac_data(geo,url=\"https://planetarycomputer.microsoft.com/api/stac/v1\",name=\"sentinel-2-l2a\",res=30,crs=5070,**kwarg):\n",
    "    '''\n",
    "    gets tiled data from planetary computer as a dask backed xarray that intersects the geometry of the point, line, or polygon\n",
    "\n",
    "    geo = (polygon) geometry bounding box (WGS84)\n",
    "    url = (string) base url to planetary computer https://planetarycomputer.microsoft.com/api/stac/v1\n",
    "    name = (string) catelog resource\n",
    "    qry =  (dictoinary) of property values {'eo:cloud_cover':{'lt':1}}\n",
    "    res = (tuple of numbers) output resolution (x,y)\n",
    "    crs = (int) output crs\n",
    "    dt = (string) data time intervale e.g., one month: 2023-06, range: 2023-06-02/2023-06-17\n",
    "    limit = (int) max number of items to return\n",
    "\n",
    "    returns (xarray data array and stac item catalog)\n",
    "    '''\n",
    "    catalog = pystac_client.Client.open(url, modifier=planetary_computer.sign_inplace)\n",
    "    srch = catalog.search(collections=name, intersects=geo, **kwarg)\n",
    "    ic = srch.item_collection()\n",
    "    if(len(ic.items)>0):\n",
    "        xra = stackstac.stack(ic,resolution=res,epsg=crs)\n",
    "        xra = mosaic_stac(xra)\n",
    "    else:\n",
    "        xra=None\n",
    "\n",
    "    return xra,ic\n",
    "\n",
    "# Create definition for WCS download\n",
    "def get_wcs_data(url,ply,service_name='mrlc_download__nlcd_tcc_conus_2021_v2021-4',out_prefix = 'tcc'):\n",
    "    '''\n",
    "    Extracts saves an image from a WCS given url, polygon boundary, and service name. Images are saved in the same location as the notebook.\n",
    "    url = (string) path to wcs e.g. 'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
    "    ply = (geoseries or geodataframe) of the study area\n",
    "    service_name = (string) name of the service e.g. mrlc_download__nlcd_tcc_conus_2021_v2021-4\n",
    "    out_prefix = (string) prefix used to save each image\n",
    "\n",
    "    returns a Raster object\n",
    "    '''\n",
    "    wcs=WebCoverageService(url)\n",
    "    tcc=wcs.contents[service_name]\n",
    "    bbox=tuple(ply.total_bounds)\n",
    "    subsets=[('X',bbox[0],bbox[2]),('Y',bbox[1],bbox[3])]\n",
    "    rsp=wcs.getCoverage(identifier=[tcc.id],subsets=subsets,format='geotiff')\n",
    "    outpath='./'+out_prefix+'.tif'\n",
    "    with open(outpath,'wb') as file:\n",
    "        file.write(rsp.read())\n",
    "\n",
    "    return Raster(outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lORu4UOw_7y"
   },
   "source": [
    "Download the data and create the raster objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aIS1S--Fw_7y"
   },
   "outputs": [],
   "source": [
    "#get stac data landsat data\n",
    "if(not os.path.exists('ls82016.tif')):\n",
    "    xmin,ymin,xmax,ymax=nf1p.buffer(200).total_bounds\n",
    "    ls30, ic =get_stac_data(nf1.geometry[0],\"https://planetarycomputer.microsoft.com/api/stac/v1\",name=\"landsat-c2-l2\",res=30,crs=5070,datetime='2016-06-15/2016-06-30',query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},limit=1000)\n",
    "    ls30s=Raster(ls30.sel(band=['red', 'green', 'blue','nir08', 'lwir11','swir16', 'swir22'],x=slice(xmin,xmax),y=slice(ymax,ymin)))\n",
    "    ls30s=ls30s.save('ls82016.tif')\n",
    "\n",
    "#Load the Landsat raster\n",
    "ls30s=Raster('ls82016.tif')\n",
    "\n",
    "#get TCC data\n",
    "if(not os.path.exists('cf1.tif')): #if the 2016 tree canopy cover file does not exits, download it\n",
    "    url=r'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2016_v2021-4/wcs?'\n",
    "    sn='mrlc_download__nlcd_tcc_conus_2016_v2021-4'\n",
    "    get_wcs_data(url=url,ply=nf1p,service_name=sn,out_prefix='cf1')\n",
    "\n",
    "\n",
    "#Load the TCC raster\n",
    "tcc_rs=Raster('cf1.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSnEqLbbw_7z"
   },
   "source": [
    "### Visualize the boundary and  imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=nf1p.plot(edgecolor='red',facecolor='none',figsize=(15,10),linewidth=5)\n",
    "p=ls30s.get_bands([1,2,3]).xdata.plot.imshow(ax=p,robust=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure 2.__ Overlay of Landsat 8 image subset (RGB bands) and the study area boundary outline in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uK7owb-w_7z"
   },
   "source": [
    "### Step 2. Create Sample\n",
    "Create definitions to sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X1ZotYGhw_7z"
   },
   "outputs": [],
   "source": [
    "import shapely, xarray as xr, numba as nb\n",
    "\n",
    "#Create a random sample definition\n",
    "def get_random_sample(bnd, n=1000):\n",
    "    \"\"\"\n",
    "    produces a random sample given a geometry\n",
    "\n",
    "    parameters:\n",
    "    bnd = (GeoDataFrame or GeoSeries) project boundary\n",
    "    n = number of observations\n",
    "\n",
    "    returns: geodataframe of point locations\n",
    "\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = bnd.total_bounds\n",
    "    xdif = xmax - xmin\n",
    "    ydif = ymax - ymin\n",
    "    pnts_lst = []\n",
    "    ubnd=bnd.unary_union\n",
    "    while len(pnts_lst) < n:\n",
    "        x = (np.random.random() * xdif) + xmin\n",
    "        y = (np.random.random() * ydif) + ymin\n",
    "        pnt = shapely.geometry.Point([x, y])\n",
    "        if pnt.intersects(ubnd):\n",
    "            pnts_lst.append(pnt)\n",
    "\n",
    "    dic = {\"geometry\": pnts_lst}\n",
    "    gdf = gpd.GeoDataFrame(dic, crs=bnd.crs)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "@nb.jit(nopython=True, nogil=True)\n",
    "def _expand_pnts_for_kernel(isys,isxs,wsize):\n",
    "    '''\n",
    "    Extracts values for kernel cells. Cells indices falling on the boundary of the image are moved in one index value.\n",
    "    isys=array of row index locations\n",
    "    isxs=array of column index locations\n",
    "    wsize=width of the kernel\n",
    "    \n",
    "    returns two new lists of index values kernel cell locations that can be used to extract coordinate from an xarray data array\n",
    "    '''\n",
    "    hw=int(wsize/2)\n",
    "    isys2=np.zeros(isys.shape[0]*wsize*wsize,dtype='int32')\n",
    "    isxs2=np.zeros(isxs.shape[0]*wsize*wsize,dtype='int32')\n",
    "    cnt=0\n",
    "    for r in range(isys.shape[0]):\n",
    "        rvl=isys[r]\n",
    "        cvl=isxs[r]\n",
    "        rvlm=rvl-hw\n",
    "        for r2 in range(wsize):\n",
    "            nr=rvlm+r2\n",
    "            cvlm=cvl-hw\n",
    "            for c2 in range(wsize):\n",
    "                nc=cvlm+c2\n",
    "                isys2[cnt]=nr\n",
    "                isxs2[cnt]=nc\n",
    "                cnt+=1\n",
    "\n",
    "    return isys2,isxs2\n",
    "\n",
    "def create_sample(bndr,r_rs,pred_rs,n,wsize=1):\n",
    "    '''\n",
    "    Creates a simple random sample of locations given a boundary and n and extracts response and predictor variables values for those location and the cell surrounding those locations.\n",
    "    bndr = GeoDataFrame or GeoSeries of the boundary\n",
    "    r_rs = response Raster object to be sampled\n",
    "    pred_rs= predictor Raster object to be sampled\n",
    "    n = (integer) sample size\n",
    "    wsize=(int) width of a square kernel in cells\n",
    "\n",
    "    returns a GeoDataFrame of response and predictor cell values\n",
    "    '''\n",
    "    pnts=get_random_sample(bndr,n=n)\n",
    "    xs=pnts.geometry.x\n",
    "    ys=pnts.geometry.y\n",
    "    rws=xs.shape[0]\n",
    "\n",
    "    #get response values\n",
    "    isys,isxs=r_rs.index(xs,ys)\n",
    "    rsel=r_rs.xdata.isel(x=xr.DataArray(isxs,dims='loc'),y=xr.DataArray(isys,dims='loc'))\n",
    "    rvls=pd.DataFrame(rsel.values.reshape((rws,1)),columns=['response'])\n",
    "\n",
    "    #get predictor values\n",
    "    isys,isxs=pred_rs.index(xs,ys)\n",
    "\n",
    "\n",
    "\n",
    "    if(wsize>1):\n",
    "        isys,isxs=_expand_pnts_for_kernel(isys.values,isxs.values,wsize)\n",
    "\n",
    "    sel=pred_rs.xdata.isel(x=xr.DataArray(isxs,dims='loc'),y=xr.DataArray(isys,dims='loc'))\n",
    "    pvls=sel.values.reshape((pred_rs.shape[0],rws,wsize*wsize))\n",
    "    clms=sel.shape[0]*wsize*wsize\n",
    "    pvls=pd.DataFrame(np.moveaxis(pvls,1,0).reshape(rws,clms),columns=['p' + str(sub) for sub in np.arange(clms)])\n",
    "\n",
    "    df=pd.concat([pnts,pvls,rvls],axis=1)\n",
    "    vls=df.dropna()#(df[df!=r_rs.null_value]).dropna()\n",
    "\n",
    "    return vls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of 150 random locations with a 5 by 5 window size.\n",
    "ksize=3\n",
    "pnts=create_sample(nf1p,r_rs=tcc_rs,pred_rs=ls30s,n=150,wsize=ksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Gradient Boosted Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(subsample=0.66,random_state=0,learning_rate=0.1)\n",
    "tdf_train = pnts.sample(110)\n",
    "tdf_val=pnts.iloc[~pnts.index.isin(tdf_train.index)].copy()\n",
    "pred_clm=tdf_train.columns[1:-1]\n",
    "X=tdf_train[pred_clm].values\n",
    "y=tdf_train['response'].values\n",
    "reg.fit(X,y)\n",
    "\n",
    "pred=reg.predict(tdf_val[pred_clm].values)\n",
    "tdf_val['pred']=pred\n",
    "tdf_train['pred']=reg.predict(tdf_train[pred_clm].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at model fit using training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "print('Training Data')\n",
    "print('Response std =',tdf_train['response'].std())\n",
    "evdf=tdf_train[['response','pred']]\n",
    "print('RMSE =',np.sqrt(((evdf['response']-evdf['pred'])**2).mean()))\n",
    "print('train r2 =',(evdf.corr()).iloc[0,1]**2)\n",
    "p = evdf.plot(kind='scatter',x='pred',y='response',title=\"Train Data Observed vs Predicted\",figsize=(8,8))\n",
    "p.axline((0, 0), slope=1,color='grey')\n",
    "display(p)\n",
    "\n",
    "#validation dataset\n",
    "print('Response std =',tdf_val['response'].std())\n",
    "evdf=tdf_val[['response','pred']]\n",
    "print('RMSE =',np.sqrt(((evdf['response']-evdf['pred'])**2).mean()))\n",
    "print('val r2 =',(evdf.corr()).iloc[0,1]**2)\n",
    "p = evdf.plot(kind='scatter',x='pred',y='response',title=\"Validation Data Observed vs Predicted\",figsize=(8,8))\n",
    "p.axline((0, 0), slope=1,color='grey')\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at variable importance fo the top ten features. \n",
    "In this case which cells are contributing most to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=pd.Series(reg.feature_importances_)\n",
    "imp2=imp.sort_values(ascending=False)\n",
    "print(imp2[:10])\n",
    "imp2.iloc[:10].plot(kind='barh',figsize=(8,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply to Landsat predictor surfaces\n",
    "#### Create definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, nogil=True)\n",
    "def _conv_flatten(x, size):\n",
    "    '''\n",
    "    reshapes the array for model implementation\n",
    "\n",
    "    x=(numpy array) of data\n",
    "    size= (int) width of the kernel\n",
    "\n",
    "    returns a numpy array of correct shape for models\n",
    "    '''\n",
    "    bnd,rws,clms=x.shape\n",
    "    \n",
    "    hs=int(size/2)\n",
    "    outarr=np.zeros(((rws-(2*hs))*(clms-(2*hs)),bnd*size*size))\n",
    "    cnt=0\n",
    "    for ri in range(hs,rws-hs):\n",
    "        sr=ri-hs\n",
    "        for ci in range(hs,clms-hs):\n",
    "            sc=ci-hs\n",
    "            vls=x[:,sr:sr+size,sc:sc+size].flatten()\n",
    "            vls=np.nan_to_num(vls,0) #set null values to zero\n",
    "            outarr[cnt]=vls\n",
    "            cnt+=1\n",
    "\n",
    "    return outarr\n",
    "\n",
    "def _conv_model(X,mdl,wsize,outbnds=1):\n",
    "    ''' \n",
    "    Applies a model developed from convolution kernel cell values to a specified array. Used with dask's map_overlap function.\n",
    "    \n",
    "    X = (array) numpy array\n",
    "    mdl = (object) model object with a predict function\n",
    "    wsize = (int) kernel width\n",
    "    outbnds = (int) number of output bands\n",
    "    '''\n",
    "    bnd,rws,clms=X.shape\n",
    "    hw=int(wsize/2)*2\n",
    "    nx=_conv_flatten(X,wsize)\n",
    "    outvl=mdl.predict(nx)\n",
    "\n",
    "    return outvl.reshape((outbnds,rws-hw,clms-hw))\n",
    "\n",
    "def conv_mdl(rs,mdl,ksize,outbands=1):\n",
    "    ''' \n",
    "    Creates a raster surface for a given model derived from the values within a sample of locations and neighboring cell values. Neighboring cell values are determined based on the size of a convolution kernel.\n",
    "    rs = (Raster) input dataset used to train the model\n",
    "    mdl =  a model with a predict function\n",
    "    ksize = the width of the convolution kernel\n",
    "    outbands = the number of output bands from the model\n",
    "\n",
    "    returns a raster dataset of predicted values\n",
    "    '''\n",
    "    hw=int(ksize/2)\n",
    "    rs2=rs.chunk((rs.nbands,*rs.data.chunks[1:]))\n",
    "    nch=rs2.data.chunks\n",
    "    och=list(nch)\n",
    "    och[0]=(outbands,)\n",
    "    och[1]=tuple(och[1])\n",
    "    och[2]=tuple(och[2])\n",
    "    #use map overlap function to retrieve kernel cell values\n",
    "    darr = rs2.data.map_overlap(\n",
    "        _conv_model,\n",
    "        depth={0: 0, 1: hw, 2: hw},\n",
    "        chunks=och,\n",
    "        trim=False,\n",
    "        boundary=0,\n",
    "        dtype='f8',\n",
    "        meta=np.array((),dtype='f8'),\n",
    "        mdl=mdl,\n",
    "        wsize=ksize,\n",
    "        outbnds=outbands\n",
    "    )\n",
    "    #convert dask array back to a raster\n",
    "    out_rs=raster.data_to_raster(darr,mask=rs.mask[0:outbands],x=rs2.x,y=rs2.y,affine=rs2.affine,crs=rs2.crs).chunk((1,*darr.chunks[1:]))\n",
    "    return out_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the model and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcc2_rs=conv_mdl(ls30s,reg,ksize,outbands=1)\n",
    "display(tcc2_rs.xdata)\n",
    "tcc2_rs.explore(band=1,cmap='PRGn') #purple to green -> low to high TCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare against actual TCC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif=(tcc_rs-tcc2_rs.reproject(tcc_rs.geobox))\n",
    "print('RMSE =',np.sqrt(np.mean(dif*dif)).compute())\n",
    "\n",
    "dif.plot(figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "### In this notebook we have demonstrated how to use simple random sampling, neighborhood windows, and Gradient Boosted Regression Trees to estimate tree canopy cover (TCC) from multi-band Landsat 8 data. Notably, of our example emphasize both hue and textural aspects within the data that are uniquely optimized to estimate TCC, given the training data. Intuitively, some band and cell combinations have more importance than others (feature importance graphic). However, allowing the modeling technique to \"weight\" each band\\cell combination gives the optimization methodology the flexibility to learn patterns of hue and texture that strongly relate to TCC. \n",
    "\n",
    "### Some additional ways to improve the model may be to adjust modeling parameters, include additional data (elevation data), modify the window size, transform the data to independent components before using as predictor surfaces, and\\or improve the spread and balance of the sample. Alternatively, a different modeling approach could potentially reduce modeling error. However, this notebook demonstrates how can use Raster Tools and neighborhood windows in a manner that reduces estimation error in a efficient, practical way.  \n",
    "               "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rstools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
