{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jshogland/SpatialModelingTutorials/blob/main/Notebooks/ShawneeNF_TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv69joTwdFaA"
   },
   "source": [
    "# Percent Tree Canopy Cover Notebook\n",
    "## In this notebook we will explore how to build a simple random sample of point locations, extract remotely sensed data for those locations, and use that data to estimate % Tree Cover for the Shawnee National Forest.\n",
    "\n",
    "### Project objective:\n",
    "- Build a machine learning model from estimates of 2016 percent forest canopy cover, elevation data, and Landsat 8 imagery for that same time period\n",
    "- Apply that model to elevation and Landsat 8 data acquired in 2021 to estimate percent forest canopy cover in 2021\n",
    "- Compare our model estimates to [MLRC](https://www.mrlc.gov/data-services-page) % forest canopy cover estimates for the year 2021\n",
    "\n",
    "#### Notebook Sections:\n",
    "1. Setup\n",
    "2. Create a Sample\n",
    "3. Download data\n",
    "4. Creating the Data Frame\n",
    "5. Clean data\n",
    "6. Preprocessing\n",
    "7. Build a predictive model\n",
    "8. Clustering and estimating % Tree Canopy Cover and Forest Density Classes\n",
    "9. Applying our models to 2021 data\n",
    "10. Making a dashboard\n",
    "\n",
    "#### Data sources\n",
    "- [Landsat 8](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) - Extracted from Planetary Computer ([STAC](https://stacspec.org/en))\n",
    "- [Tree Canopy Cover 2016](https://apps.fs.usda.gov/fsgisx01/rest/services/RDW_LandscapeAndWildlife/NLCD_2016_TreeCanopyCover_CONUS/ImageServer) - Extracted from Landfire Image Service ([REST](https://developers.arcgis.com/rest/services-reference/enterprise/image-service/))\n",
    "- [USGS Elevation](https://www.usgs.gov/3d-elevation-program) - Extracted from USGS 3DEP program ([py3dep](https://github.com/hyriver/py3dep))\n",
    "- [National Forest Boundary](https://www.openstreetmap.org/#map=9/34.306/-112.242) - Extracted from Open Street Maps ([osmnx](https://osmnx.readthedocs.io/en/stable/))\n",
    "\n",
    "Author John Hogland 10/1/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdouEjUmdFaB"
   },
   "source": [
    "### Section 1: Setup\n",
    "#### Installing software for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdqdQ1_PdFaB"
   },
   "outputs": [],
   "source": [
    "!pip install mapclassify\n",
    "!pip install osmnx\n",
    "!pip install raster_tools\n",
    "!pip install planetary-computer\n",
    "!pip install pystac-client\n",
    "!pip install stackstac\n",
    "!pip install py3dep==0.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIuHENindFaB"
   },
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTkFqp2LdFaC"
   },
   "outputs": [],
   "source": [
    "#get packages\n",
    "import osmnx as ox, planetary_computer, pystac_client, stackstac\n",
    "import geopandas as gpd, pandas as pd, os, numpy as np, requests, urllib, py3dep\n",
    "\n",
    "from raster_tools import Raster,zonal, general, Vector\n",
    "from shapely import geometry\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGSrvI_VdFaC",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Section 2: Create a sample\n",
    "#### In this section we will extract the spatial boundary of the Shawnee National Forest using Open Street Maps (OSM) Web Service and create a simple random sample of locations within the boundary of the National forest.\n",
    "\n",
    "#### Key learning points:\n",
    "- Using OSM to get vector data (downloading)\n",
    "- Generating a simple random sample of point location that will be used to create a data frame of response and predictor variables\n",
    "- How to Visualize the the boundary and point locations\n",
    "\n",
    "#### Coding Steps:\n",
    "1. Use osmnx and OSM to get a GeoDataFrame (polygons) of the boundary of the Shawnee National Forest\n",
    "2. Use the Shawnee National Forest boundary and Geopanda's sample_points function to create a simple random sample of point locations\n",
    "3. Create a interactive map that displays our point locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJhsSKWPdFaC"
   },
   "source": [
    "##### Step 1: Get the Shawnee National Forest boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-OrUepEdFaC"
   },
   "outputs": [],
   "source": [
    "#use osmnx OpenStreetMaps to get the boundary of the NF (GeoDataFrame)\n",
    "nf=ox.geocode_to_gdf('Shawnee National Forest, IL, USA')\n",
    "\n",
    "#Look at the coordinate system of the GeoDataFrame\n",
    "print('Original CRS =',nf.crs)\n",
    "\n",
    "#project the GeoDataFrame to Albers equal area EPSG:5070\n",
    "nfp=nf.to_crs(5070)\n",
    "\n",
    "#Look at the GeoDataFrame\n",
    "display(nfp)\n",
    "\n",
    "#Plot the national forest\n",
    "nfp.plot(figsize=(15,15))\n",
    "\n",
    "#Why are we projecting to Albers equal Area?\n",
    "\n",
    "#How many acres is the National Forest?\n",
    "print('Number of acres =',(nfp.area * 0.000247105).values[0])\n",
    "\n",
    "#How many polygons are in the National Forest?\n",
    "print('Number of multipolygons =',nfp.shape[0])\n",
    "print('Number of polygons =',nfp.explode().shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_SmIuf2dFaC"
   },
   "source": [
    "##### Step 3: Create the simple random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GT6iqyxdFaC"
   },
   "outputs": [],
   "source": [
    "#us random sample function to create 2,000 locations within the nfp\n",
    "mpnts=nfp.sample_points(2000)\n",
    "\n",
    "#Look at the GeoSeries\n",
    "mpnts\n",
    "\n",
    "#Why 2,000 locations? Why not 100 or 10,000?\n",
    "\n",
    "#How many records do we have?\n",
    "#print('Number of records =',mpnts.shape[0])\n",
    "\n",
    "#How many observations do we have?\n",
    "#print('Number of points (observations) =',mpnts.explode().shape[0])\n",
    "\n",
    "#How can we convert our multipoint to points?\n",
    "pnts=mpnts.explode().reset_index(drop=True) #reseting the index to clean things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elx0kt__dFaC"
   },
   "source": [
    "##### Step 4: Visualize the national forest boundary and point locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHmkhbhXdFaD"
   },
   "outputs": [],
   "source": [
    "#Use geopandas explore function to create a interactive map\n",
    "m=nfp.explore(color='blue') #create the map using the National Forest Boundary\n",
    "m=pnts.explore(m=m,color='yellow') #add out points to the map\n",
    "m #show the map\n",
    "\n",
    "#Can we save our map to share with others?\n",
    "#m.save('sample.html')\n",
    "\n",
    "#How can we plot our map?\n",
    "# p=nfp.plot(edgecolor='blue',facecolor='none',figsize=(15,15))\n",
    "# p=pnts.plot(ax=p,color='yellow')\n",
    "# p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq-9HKYGdFaD"
   },
   "source": [
    "### Section 3: Downloading Raster Data\n",
    "#### In this section we will focus on downloading raster dataset from various sources.\n",
    "\n",
    "#### Key learning points:\n",
    "- Accessing data from different web services\n",
    "- Deciding where to do your processing (client or server side)\n",
    "- How and when to store the data locally\n",
    "- How to visualizing the raster surfaces\n",
    "\n",
    "#### Coding Steps:\n",
    "1. Create functions to download data from STAC, REST, WCS\n",
    "2. Use those functions to create Raster datasets\n",
    "3. Visualize Your Raster datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-4qM4HJdFaD"
   },
   "source": [
    "##### Step 1: Creating download definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMe0XcOadFaD"
   },
   "outputs": [],
   "source": [
    "from owslib.wcs import WebCoverageService\n",
    "#create definition to mosaic stac data\n",
    "def mosaic_stac(xr):\n",
    "    '''\n",
    "    Creates a mosaic from multi-temporal xarray stack\n",
    "\n",
    "    xr=Xarray object\n",
    "\n",
    "    returns xarray mosaic\n",
    "    '''\n",
    "    return stackstac.mosaic(xr)\n",
    "\n",
    "#create definition to extract stac data\n",
    "def get_stac_data(ply,url,name,bands,res=30,crs=5070,\n",
    "                  out_prefix='ls8',**kwarg):\n",
    "    '''\n",
    "    gets tiled data from planetary computer as a dask backed xarray that intersects the geometry of the point, line, or polygon\n",
    "\n",
    "    ply = (geoseries or geodataframe) of the study area\n",
    "    url = (string) base url to planetary computer https://planetarycomputer.microsoft.com/api/stac/v1\n",
    "    name = (string) catelog resource e.g., \"sentinel-2-l2a\"\n",
    "    bands = (list(string)) of attributes ['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
    "    qry =  (dictionary) of property values {'eo:cloud_cover':{'lt':1}}\n",
    "    res = (tuple of numbers) output resolution (x,y)\n",
    "    crs = (int) output crs\n",
    "    dt = (string) data time intervale e.g., one month: 2023-06, range: 2023-06-02/2023-06-17\n",
    "    limit = (int) max number of items to return\n",
    "    out_prefix = (string) prefix used to save the image\n",
    "\n",
    "    returns a Raster object\n",
    "    '''\n",
    "    geo = ply.to_crs('4326').envelope.geometry[0]\n",
    "    xmin,ymin,xmax,ymax=ply.total_bounds\n",
    "    catalog = pystac_client.Client.open(url, modifier=planetary_computer.sign_inplace)\n",
    "    srch = catalog.search(collections=name, intersects=geo, **kwarg)\n",
    "    ic = srch.item_collection()\n",
    "    if(len(ic.items)>0):\n",
    "        xra = stackstac.stack(ic,resolution=res,epsg=crs)\n",
    "        xra = mosaic_stac(xra)\n",
    "        rs=Raster(xra.sel(band=bands,x=slice(xmin,xmax),y=slice(ymax,ymin)))\n",
    "        outpath=out_prefix+'.tif'\n",
    "        rs.save(outpath)\n",
    "        rs=Raster(outpath)\n",
    "    else:\n",
    "        rs=None\n",
    "\n",
    "    return rs\n",
    "\n",
    "#Create definition to extract image service data\n",
    "def get_image_service_data(url, ply, out_prefix,res=30,outSR=\"\"):\n",
    "    '''\n",
    "    extracts a list of images from a image service given a url, polygon, and output prefix name\n",
    "\n",
    "    url = (string) path to image service e.g., url=r'https://lfps.usgs.gov/arcgis/rest/services/Landfire_LF230/US_230EVT/ImageServer'\n",
    "    ply = (geoseries or geodataframe) of the study area\n",
    "    out_prefix = (string) prefix used to save each image\n",
    "\n",
    "    returns a list of Raster objects, one for each tile\n",
    "    '''\n",
    "    layerInfo=requests.get(url+'?f=pjson')\n",
    "    dic=layerInfo.json()\n",
    "    #print(dic)\n",
    "    spr=dic['spatialReference']\n",
    "    m_width=dic['maxImageWidth']\n",
    "    m_height=dic['maxImageHeight']\n",
    "    fitem=next(iter(spr))\n",
    "    ply2=ply.to_crs(spr[fitem])\n",
    "\n",
    "    xmin,ymin,xmax,ymax=ply2.total_bounds\n",
    "\n",
    "    wcells=int((xmax-xmin)/res)\n",
    "    hcells=int((ymax-ymin)/res)\n",
    "\n",
    "    if(wcells<m_width):\n",
    "        m_width=wcells\n",
    "\n",
    "    if(hcells<m_height):\n",
    "        m_height=hcells\n",
    "\n",
    "\n",
    "    wcells_l=np.arange(0,wcells,m_width)\n",
    "    hcells_l=np.arange(0,hcells,m_height)\n",
    "\n",
    "    xmax2=xmin\n",
    "    ymax2=ymin\n",
    "\n",
    "    tile=1\n",
    "\n",
    "    rs_lst=[]\n",
    "    for w in wcells_l:\n",
    "        for h in hcells_l:\n",
    "            xmax2 = (m_width*res+xmax2)\n",
    "            ymax2 = (m_height*res+ymax2)\n",
    "\n",
    "            qry = url+'/exportImage?'\n",
    "            parm = {\n",
    "                'f':'json',\n",
    "                'bbox':','.join([str(xmin),str(ymin),str(xmax2),str(ymax2)]),\n",
    "                'size':str(m_width) + ',' + str(m_height),\n",
    "                'imageSR':outSR,\n",
    "                'format':'tiff'\n",
    "            }\n",
    "            #print(parm['bbox'])\n",
    "            response=requests.get(qry,parm)\n",
    "            if response.status_code == 200:\n",
    "                img_url=response.json()['href']\n",
    "                outname=out_prefix + str(tile) + '.tif'\n",
    "                urllib.request.urlretrieve(img_url, outname)\n",
    "                rs_lst.append(Raster(outname))\n",
    "                tile+=1\n",
    "\n",
    "    return rs_lst\n",
    "\n",
    "# Creat definition for WCS download\n",
    "def get_wcs_data(url,ply,service_name='mrlc_download__nlcd_tcc_conus_2021_v2021-4',out_prefix = 'tcc'):\n",
    "    '''\n",
    "    Extracts saves an image from a WCS given url, polygon boundary, and service name. Images are saved in the same location as the notebook.\n",
    "    url = (string) path to wcs e.g. 'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
    "    ply = (geoseries or geodataframe) of the study area\n",
    "    service_name = (string) name of the service e.g. mrlc_download__nlcd_tcc_conus_2021_v2021-4\n",
    "    out_prefix = (string) prefix used to save each image\n",
    "\n",
    "    returns a Raster object\n",
    "    '''\n",
    "    wcs=WebCoverageService(url)\n",
    "    tcc=wcs.contents[service_name]\n",
    "    bbox=tuple(ply.total_bounds)\n",
    "    subsets=[('X',bbox[0],bbox[2]),('Y',bbox[1],bbox[3])]\n",
    "    rsp=wcs.getCoverage(identifier=[tcc.id],subsets=subsets,format='geotiff')\n",
    "    outpath='./'+out_prefix+'.tif'\n",
    "    with open(outpath,'wb') as file:\n",
    "        file.write(rsp.read())\n",
    "\n",
    "    return Raster(outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMavd0F0dFaD"
   },
   "source": [
    "##### Step 2: Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGwNO9h7dFaD"
   },
   "outputs": [],
   "source": [
    "# #Get Landsat data from STAC\n",
    "outpath='ls82016.tif'\n",
    "fnd=True\n",
    "if(not os.path.exists(outpath)): #if the 2016 Landsat file does not exits, download it\n",
    "    ls_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "    ls_name = \"landsat-c2-l2\"\n",
    "    bnds=['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
    "    ck=get_stac_data(nfp,ls_url,\n",
    "                    name=ls_name,bands=bnds,\n",
    "                    res=30,crs=5070,out_prefix='ls82016',datetime='2016-06-01/2016-06-30', #date time is very important here\n",
    "                    query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},\n",
    "                    limit=1000)\n",
    "    if(ck is None): fnd=False\n",
    "\n",
    "if fnd: ls82016=Raster(outpath)\n",
    "else: \"No objects found in the query\"\n",
    "\n",
    "# # #Get 2016 TCC data from Image Service\n",
    "# outpath='tcc20161.tif'\n",
    "# if(not os.path.exists(outpath)): #if the 2016 tree canopy cover file does not exits, download it\n",
    "#     url=r'https://apps.fs.usda.gov/fsgisx01/rest/services/RDW_LandscapeAndWildlife/NLCD_2016_TreeCanopyCover_CONUS/ImageServer'\n",
    "#     im_lst=get_image_service_data(url=url,ply=nfp,out_prefix='tcc2016',res=30,outSR=5070)\n",
    "\n",
    "# tcc2016=Raster(outpath) #should only be one tile\n",
    "\n",
    "outpath='tcc2016.tif'\n",
    "if(not os.path.exists(outpath)): #if the 2016 tree canopy cover file does not exits, download it\n",
    "    url=r'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2016_v2021-4/wcs?'\n",
    "    sn='mrlc_download__nlcd_tcc_conus_2016_v2021-4'\n",
    "    get_wcs_data(url=url,ply=nfp,service_name=sn,out_prefix='tcc2016')\n",
    "\n",
    "tcc2016=Raster(outpath) #should only be one tile\n",
    "\n",
    "\n",
    "#Get elevation data from USFS 3Dep program web service\n",
    "outpath='./dem.tif' #specify the output path\n",
    "if(not os.path.exists(outpath)): #if the dem file does not exist, download it\n",
    "    geo = nf.envelope.geometry[0] #get the geometry for extent of the study area\n",
    "    d1=py3dep.get_dem(geo,30,4326).expand_dims({'band':1}) # use py3dep to get a xarray surface\n",
    "    d2=Raster(d1).reproject(nfp.crs) #convert d1 into a Raster object\n",
    "    d2.save(outpath) #save it to disk\n",
    "\n",
    "dem=Raster(outpath) #load your dem from disk\n",
    "\n",
    "#Don't we also need Landsat imagery for the year 2021? How can we get that?\n",
    "# outpath='ls82021.tif'\n",
    "# fnd=True\n",
    "# if(not os.path.exists(outpath)): #if the 2016 Landsat file does not exits, download it\n",
    "#     ls_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "#     ls_name = \"landsat-c2-l2\"\n",
    "#     bnds=['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
    "#     ck=get_stac_data(nfp,ls_url,\n",
    "#                     name=ls_name,bands=bnds,\n",
    "#                     res=30,crs=5070,out_prefix='ls82016',datetime='2021-06-01/2021-06-30', #date time is very important here\n",
    "#                     query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},\n",
    "#                     limit=1000)\n",
    "#     if(ck is None): fnd=False\n",
    "\n",
    "# if fnd: ls82016=Raster(outpath)\n",
    "# else: \"No objects found in the query\"\n",
    "\n",
    "#TCC for 2021 does not exist on Landfire's image service. How can we get TCC for 2021?\n",
    "# outpath='tcc2021.tif'\n",
    "# if(not os.path.exists(outpath)): #if the 2021 tree canopy cover file does not exits, download it\n",
    "#     url=r'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
    "#     sn='mrlc_download__nlcd_tcc_conus_2021_v2021-4'\n",
    "#     get_wcs_data(url=url,ply=nfp,service_name=sn,out_prefix='tcc2021')\n",
    "\n",
    "# tcc2021=Raster(outpath) #should only be one tile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT7Ue-vEdFaD"
   },
   "source": [
    "##### Step3: Visualize the Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKQvuEcgdFaD"
   },
   "outputs": [],
   "source": [
    "#Use Raster Tools to display a web map of the first band in the Landsat image\n",
    "display(ls82016.xdata)\n",
    "display(ls82016.explore(band=1,cmap='Reds'))\n",
    "\n",
    "#How can we do the same thing for our tree canopy cover and dem surfaces?\n",
    "# display(tcc2016.xdata)\n",
    "# display(tcc2016.explore(band=1,cmap='Greens'))\n",
    "# display(dem.xdata)\n",
    "# dem.explore(band=1,cmap='terrain')\n",
    "\n",
    "#Can we visualize the Landsat image as a 3 band color composite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W89xPRrsdFaD"
   },
   "source": [
    "### Section 4: Creating the Data Frame\n",
    "#### In this section we will focus on creating a data frame for further analyses.\n",
    "\n",
    "#### Key learning points:\n",
    "- The utility of data frame\n",
    "- How to use both Raster and Vector data\n",
    "- How to visualize the raw data\n",
    "- How to store the data\n",
    "\n",
    "#### Coding Steps:\n",
    "1. Extract spectral, elevation, and tcc values for each point\n",
    "2. Merge data with points\n",
    "3. Look at the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZK58AZLdFaD"
   },
   "source": [
    "##### Step 1: Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5cRc22KdFaD"
   },
   "outputs": [],
   "source": [
    "lstbl=zonal.extract_points_eager(pnts,ls82016,'ls',axis=1).compute()\n",
    "eltbl=zonal.extract_points_eager(pnts,dem,'el',axis=1).compute()\n",
    "tcctbl=zonal.extract_points_eager(pnts,tcc2016,'tcc',axis=1).compute()\n",
    "tcctbl.loc[tcctbl['tcc_1'].isnull(),'tcc_1']=0 #set null values to zero canopy cover (first cleaning step)\n",
    "\n",
    "#why do we need to compute?\n",
    "#why did we set null values to zero?\n",
    "#what does axis=1 mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjD01K1adFaE"
   },
   "source": [
    "##### Step 2: Merge data with points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vekNS31cdFaE"
   },
   "outputs": [],
   "source": [
    "atr=pd.concat([lstbl,eltbl,tcctbl],axis=1)\n",
    "gdf=gpd.GeoDataFrame(atr,geometry=pnts)\n",
    "\n",
    "#what does axis 1 mean in this context?\n",
    "\n",
    "#Why do we want to use a geodataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRbV5SJmdFaE"
   },
   "source": [
    "##### Step 3: Look at the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3B2jBTrdFaE"
   },
   "outputs": [],
   "source": [
    "display(gdf)\n",
    "gdf.explore(column='tcc_1')\n",
    "\n",
    "#How can we save our geodataframe into a format others can use?\n",
    "# gdf.to_file('data.shp.zip') # a\n",
    "# gdf.to_csv('data.csv')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpzauuhVdFaE"
   },
   "source": [
    "### Section 5: Cleaning Data\n",
    "#### In this section we will explore our data and look for problems that may need to be fixed before we proceed\n",
    "\n",
    "#### Key learning points:\n",
    "- Data are often messy\n",
    "- Missing data can be troublesome to deal with\n",
    "- Ways to identify issues with the data and fix those issues\n",
    "- plotting and graphing\n",
    "\n",
    "#### Coding Steps:\n",
    "1. Finding incorrect values, missing data, and extreme values\n",
    "2. Replacing values\n",
    "3. Dropping records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvF1vzfzdFaE"
   },
   "source": [
    "##### Step 1: Finding incorrect values or missing data\n",
    "Here we make a distinction between missing data (NA), incorrect values (typos, nodata values), and extreme values. Records with missing data or incorrect values need to be closely examined to see if they need to be removed or replaced, while records with extreme values may highlight unique instances in the data that warrant further investigation. While there are many approaches to cleaning data, here are few common techniques:\n",
    "- Look at the data for obvious errors\n",
    "- Find null values and replace them with the mean of the feature or impute the closest value\n",
    "- Find records with null values and drop those records from the dataset\n",
    "- Use percentiles to identify extreme and potentially incorrect values\n",
    "- Look at correlation\n",
    "\n",
    "Looking for obvious errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-LMvjCHdFaE"
   },
   "outputs": [],
   "source": [
    "#Create a map of locations overlaid on raster surfaces\n",
    "import folium\n",
    "m = nfp.explore(name='Boundary')\n",
    "m = gdf.explore(m=m, color='yellow',name='Points')\n",
    "\n",
    "folium.TileLayer(\n",
    "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr = 'Esri',\n",
    "        name = 'Esri Imagery',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(m)\n",
    "\n",
    "\n",
    "# comment and uncomment image layers and rerun\n",
    "m = tcc2016.explore(band=1,map=m,cmap='Greens',name='TCC')\n",
    "#m = ls82016.explore(band=1,map=m,cmap='Reds',name='Landsat Band 1')\n",
    "#m = dem.explore(band=1,map=m,cmap='terrain',name='DEM')\n",
    "\n",
    "m\n",
    "\n",
    "#How can we add all three images to the same map?\n",
    "\n",
    "#Do we see any obvious points we should exclude from our analysis?\n",
    "\n",
    "#What is our population and what does that mean with regards to inference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0l1LY5UdFaE"
   },
   "outputs": [],
   "source": [
    "#search for null values\n",
    "na_rows=gdf[gdf.isna().any(axis=1)]\n",
    "print('number of rows with na =',na_rows.shape[0])\n",
    "\n",
    "#search for empty values\n",
    "emp_rows=gdf[(gdf=='').any(axis=1)]\n",
    "print('number of rows that have empty values =',emp_rows.shape[0])\n",
    "\n",
    "#what about zero values?\n",
    "zero_rows=gdf[(gdf==0).any(axis=1)]\n",
    "print('number of rows that have zero values =',emp_rows.shape[0])\n",
    "\n",
    "# It does not look like we have any missing data. If we did, how could we remove records or populate missing values?\n",
    "\n",
    "# what about using the feature average?\n",
    "\n",
    "# what about imputation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmhi8h6udFaE"
   },
   "outputs": [],
   "source": [
    "#look at summary statistics for continuous data\n",
    "print('Summary stats for continuous variables...')\n",
    "display(gdf.describe())\n",
    "\n",
    "# use quantiles to look at extreme values (continuous)\n",
    "extreme=atr.quantile([0.01,0.99])\n",
    "ex=(atr<extreme.min()).sum(axis=1) + (atr>extreme.max()).sum(axis=1) #sum the number of extreme values in each row\n",
    "gdf['extreme'] = ex #add that attribute to gdf\n",
    "\n",
    "# calculate correlation\n",
    "print('','Correlation Matrix')\n",
    "display(atr.corr())\n",
    "\n",
    "# map extreme values\n",
    "print('Map of extreme values (blue to red increased number of extreme values)')\n",
    "display(gdf[ex>0].explore(column='extreme', cmap= 'RdBu_r'))\n",
    "\n",
    "#create a scatter plot matrix of values that highlight the extreme values\n",
    "print('Scatter Plot Matrix (blue to red increased number of extreme values)')\n",
    "display(pd.plotting.scatter_matrix(atr,figsize=(15,15),c=ex,cmap='RdBu_r'))\n",
    "\n",
    "\n",
    "# what relationships do you see?\n",
    "\n",
    "# does there appear to be any observations we should exclude from our analysis?\n",
    "\n",
    "# How can we determine which observations are influential or have leverage?\n",
    "\n",
    "# What if we extracted larger areas than just the cell?\n",
    "\n",
    "# How could we incorporate texture?\n",
    "\n",
    "# what if we created surface derivatives of elevation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example all of our data is continuous and clean. While this is ideal, it would be a good idea to address messy data issues. To do this let's make some categorical data, add a little random noise to that data, and insert null values. For our new categorical variables, one will be a response variable called forest_density with labeling errors and the other variable will be a predictor variable called red_cat with data recording errors. Finally, for our continuous predictor variables we will randomly add in null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's copy our dataframe and make a new dataframe called gdf_m\n",
    "gdf_m = gdf.copy()\n",
    "\n",
    "# Let's create forest_density with the categories Low, medium, and high based on tcc_1\n",
    "tcc=gdf_m['tcc_1']\n",
    "z=tcc==0\n",
    "l=tcc<20\n",
    "h=tcc>80\n",
    "m=(20<=tcc) & (tcc<=80)\n",
    "gdf_m.loc[l,'forest_density']='low'\n",
    "gdf_m.loc[m,'forest_density']='medium'\n",
    "gdf_m.loc[h,'forest_density']='high'\n",
    "gdf_m.loc[z,'forest_density']='non-forested' #why did I us z last? Could I have done this differently?\n",
    "\n",
    "# Let's create red_cat with categories 1=dark, 2=normal, and 3=bright based on ls_1\n",
    "ls_1=gdf_m['ls_1']\n",
    "d=ls_1<0.022\n",
    "b=ls_1>0.024\n",
    "n=(0.020<=ls_1) & (ls_1<=0.024)\n",
    "gdf_m.loc[d,'red_cat']=1\n",
    "gdf_m.loc[n,'red_cat']=2\n",
    "gdf_m.loc[b,'red_cat']=3\n",
    "\n",
    "# Let's make some messy data for forest_density (labeling error for the first 1000 records)\n",
    "fd=gdf_m['forest_density'].copy()\n",
    "fd.iloc[:1000]=fd[:1000].astype('str').str[0]\n",
    "gdf_m['forest_density']=fd\n",
    "\n",
    "# Let's make some messy data for red_cat (data recorder error, value = 0)\n",
    "rvl=gdf_m['red_cat'].copy()\n",
    "rvl.iloc[np.random.choice(np.arange(2000),10,replace=False)]=0\n",
    "gdf_m['red_cat']=rvl.astype('str')\n",
    "\n",
    "# Look at the data\n",
    "display(gdf_m)\n",
    "#look at summary statistics for categorical data\n",
    "print('Summary stats for categorical variables')\n",
    "display(gdf_m.describe(include='object'))\n",
    "\n",
    "#look at the unique categorical values\n",
    "print('Unique values and counts for forest_density')\n",
    "print(np.unique(gdf_m['forest_density'],return_counts=True))\n",
    "\n",
    "print('Unique values and counts for red_cat')\n",
    "print(np.unique(gdf_m['red_cat'],return_counts=True))\n",
    "\n",
    "# Lets make some messy data for randomly chosen continuous predictor variables that have extreme values (extreme>5)\n",
    "sdt=gdf_m[ex>5].copy()\n",
    "for r in range(sdt.shape[0]):\n",
    "    vls=sdt.iloc[r].copy()\n",
    "    c=np.random.randint(1,7,1)[0]\n",
    "    vls.iloc[c]=np.nan\n",
    "    sdt.iloc[r]=vls\n",
    "\n",
    "gdf_m.loc[ex>5]=sdt\n",
    "\n",
    "print('\\nNumber of extreme values = ',(ex>5).sum())\n",
    "print('Number of nans =',gdf_m.isna().sum().sum())\n",
    "\n",
    "#show the messy data\n",
    "print('\\nHere are the rows with nans...')\n",
    "display(gdf_m[ex>5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jarP8c3JdFaE"
   },
   "source": [
    "##### Step 2: Replacing values\n",
    "Now that we have some messy data, we can explore some commonly used ways to replace missing data; 1) using univariate statistics and 2) imputing missing data. To do this we will be using [scikit-learn's impute module](https://scikit-learn.org/stable/modules/impute.html).\n",
    "\n",
    "Let's start with the labeling errors in forest_density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need our labels to be consistent. To fix this issue let's use a mapping dictionary\n",
    "rdic={\n",
    "    'l':'low',\n",
    "    'm':'medium',\n",
    "    'h':'high',\n",
    "    'n':'non-forested'\n",
    "}\n",
    "gdf_m['forest_density']=gdf_m['forest_density'].replace(rdic) #update the forest_density column\n",
    "\n",
    "#let's look at the change\n",
    "gdf_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the easy ones taken care of, let's make 2 more copies of the data to explore different ways of addressing missing data (Simple Imputer, Iterative Imputer, and Dropping records).\n",
    "\n",
    "First let's fix the null values for continuous predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqbeUBTkdFaE"
   },
   "outputs": [],
   "source": [
    "#make 3 more copies of gdf_m. \n",
    "# gdf_m is used as the original, \n",
    "# gdf_m1 is used for Simple Imputer,\n",
    "# gdf_m2 is used for Iterative Imputer\n",
    "# gdf_m3 is used for dropping records\n",
    "gdf_m1=gdf_m.copy() \n",
    "gdf_m2=gdf_m.copy() \n",
    "gdf_m3=gdf_m.copy()\n",
    "\n",
    "#let's us scikit-learn's SimpleImputer to fill in the mean value for a nans\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#subset the data to the columns that have null or nan values\n",
    "mdata=gdf_m1[gdf_m1.columns[:7]]\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean') #Specify the SimpleImputer (mean with missing value = nan)\n",
    "imp.fit(mdata) #fit the mdata for all non nan values\n",
    "cdata=pd.DataFrame(imp.transform(mdata),columns=gdf_m1.columns[:7]) #create the new data frame with no nans\n",
    "\n",
    "#look at the changed records\n",
    "display(cdata[ex>5])\n",
    "\n",
    "#update the records in gdf_m\n",
    "gdf_m1[gdf_m1.columns[:7]]=cdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tackle the data recording errors in red_cat ('0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc=gdf_m1[['red_cat']] #setup the dataframe for scikit learn\n",
    "imp2=SimpleImputer(missing_values='0.0', strategy='most_frequent') #Specify the SimpleImputer (most_frequent with missing value= '0')\n",
    "imp2.fit(rc) #fit the rc data for all '0'\n",
    "cdata2=pd.DataFrame(imp2.transform(rc),columns=['red_cat']) #create the new data frame with no '0'\n",
    "\n",
    "#display the records that had 0.0 and look at their new values\n",
    "display(cdata2.loc[gdf_m['red_cat']=='0.0'])\n",
    "\n",
    "#update gdf_m red_cat values\n",
    "gdf_m1['red_cat']=cdata2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing with the Iterative Imputer and our continuous variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqNNfe_NdFaF"
   },
   "outputs": [],
   "source": [
    "#let's us scikit-learn's IterativeImputer to fill in the nan values\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "mdata=gdf_m2[gdf_m2.columns[:7]]\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit(mdata)\n",
    "cdata2=pd.DataFrame(imp.transform(mdata),columns=mdata.columns) #create the new data frame with no nans\n",
    "\n",
    "#look at the changed records\n",
    "display(cdata2[ex>5])\n",
    "\n",
    "#update the records in gdf_m2\n",
    "gdf_m2[gdf_m2.columns[:7]]=cdata\n",
    "\n",
    "#why are the values different?\n",
    "\n",
    "#which technique is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget red_cat. We have many options here including using the IterativeImputer but let's use a RandomForest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# First split out our data based on the records that have '0.0' value in the red_cat and that have a real label (1-3)\n",
    "categorical = 'red_cat' #identify the categorical response variable name\n",
    "pred = gdf_m2.columns[:7] #identify the predictor variables name\n",
    "\n",
    "clms=list(pred) + [categorical] #get all variable names used in the analysis\n",
    "mdata=gdf_m2[clms] #create a intermediate dataset\n",
    "\n",
    "#split the data into training data and new data we want to predict\n",
    "ndata=mdata[mdata[categorical]=='0.0'][pred] #these are the records we want to predict a label \n",
    "if(ndata.shape[0]>0): #do a quick look to see if values have alread been imputed\n",
    "    train=mdata[clms][~mdata.index.isin(ndata.index)] #these are the records we will use to train our model\n",
    "    X=train[pred] #predictor variables and values\n",
    "    y=train[categorical] #response variable and values\n",
    "\n",
    "    rf=RandomForestClassifier() #create the randomforest classifier\n",
    "    rf.fit(X,y) #fit the model using the training data\n",
    "    lbl=rf.predict(ndata) #predict the labels for the new data (records with '0.0' )\n",
    "    display(lbl) #look at the predictions\n",
    "\n",
    "    #update the database with the new labels\n",
    "    gdf_m2.loc[gdf_m2[categorical]=='0.0',categorical]=lbl\n",
    "\n",
    "#look at the changed records\n",
    "gdf_m2.loc[gdf_m[categorical]=='0.0']\n",
    "\n",
    "#How is this different than the IterativeImputer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7eD4AXMdFaF"
   },
   "source": [
    "##### Step 3: Dropping records\n",
    "What if so much of the data for a record is gone or is somehow incorrect that it really does not add anything to our analysis or worse yet detracts from our analysis? In that case we may want to think about removing that record from our dataset. While this approach should be used as a last resort, it is very easy to implement. For demonstration purposes, we will show how to select records and remove them from our geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpY2FMxfdFaF"
   },
   "outputs": [],
   "source": [
    "#let's drop all records with a nan value\n",
    "cdata=gdf_m.dropna()\n",
    "\n",
    "print('The number of rows in cdata =',cdata.shape[0])\n",
    "\n",
    "#now let's drop all records with a value of '0.0' in the red_cat column\n",
    "gdf_m3=cdata[cdata['red_cat']!='0.0'].reset_index(drop=True)\n",
    "\n",
    "print('The number of rows in gdf_m3 =',gdf_m3.shape[0])\n",
    "\n",
    "#why do we need to remove na values?\n",
    "\n",
    "#why is dropping rows used as last resort?\n",
    "\n",
    "#How can you change a know value that is incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZxkzN2ldFaF"
   },
   "source": [
    "### Section 6: Preprocessing\n",
    "#### In this section we will begin looking at ways to prepare our data for modeling. While we have 3 datasets to work with (gdf_m1, gdf_m2, and gdf_m3) that have used different cleaning methods to address missing or incorrect data, for the rest of the notebook we will be working with gdf_m3. However, it is worth comparing results against the other cleaned dataset to determine the impact of cleaning.\n",
    "\n",
    "#### Key learning points:\n",
    "- Standardizing, scaling, normalizing, and encoding data\n",
    "- Transformations and Ordination\n",
    "- Looking for patterns in the data\n",
    "\n",
    "#### Coding steps:\n",
    "1. Standardizing data\n",
    "2. Transformations and Ordination\n",
    "3. Looking for patterns in the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Standardizing, scaling, normalizing, and encoding data\n",
    "Data can have a wide range of values and meaning. To prepare data for modeling, we often need to standardize the data so that no one feature or column is more important than another. Depending on the type of data (categorical or continuous), we have multiple options when it comes to standardization, scaling, normalization, and encoding [SciKit Learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#). For example, one can normalize continuous data to have zero mean and unit variance or they can simply scale values to a range between 0 and 1. Likewise, many modeling techniques require categorical variables to be encoded in a special manner (e.g., One Hot Encoder). In this step we will demonstrate how to scale our Landsat and elevation values between 0 and 1 and use the One Hot Encoder method to encode our red_cat variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler \n",
    "\n",
    "#extract continuous variables Landsat 1-7 and elevation\n",
    "X=gdf_m3[gdf_m3.columns[:8]]\n",
    "\n",
    "#create MinMaxScaler object\n",
    "mms=MinMaxScaler()\n",
    "\n",
    "#fit the data\n",
    "mms.fit(X)\n",
    "\n",
    "#Transform the data\n",
    "sX=mms.transform(X)\n",
    "\n",
    "#Convert the transformed data to a dataframe\n",
    "sdf1=pd.DataFrame(sX,columns='sc_'+mms.get_feature_names_out())\n",
    "\n",
    "#extract the categorical variables red_cat\n",
    "X2=gdf_m3[['red_cat']]\n",
    "\n",
    "#create OneHotEncoder\n",
    "ohe=OneHotEncoder()\n",
    "\n",
    "#fit the data\n",
    "ohe.fit(X2)\n",
    "\n",
    "#Transform the data\n",
    "oheX=ohe.transform(X2).toarray()\n",
    "\n",
    "#Convert the transformed data into a dataframe\n",
    "sdf2=pd.DataFrame(oheX,columns=ohe.get_feature_names_out())\n",
    "\n",
    "#Combine the dataframes and look at the data\n",
    "gdf_m3=pd.concat([gdf_m3,sdf1,sdf2],axis=1)\n",
    "\n",
    "#Look at the dataframe\n",
    "gdf_m3\n",
    "\n",
    "# Task 1: use the Standard Scaler to standardize Landsat Values\n",
    "# Task 2: use Ordinal Encoder on forest_density\n",
    "# Task 3: please describe at least one additional technique to standardize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(gdf_m3['forest_density'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Transformation and Ordination\n",
    "Transforming and ordination of data are often employed to aid in the model building process, help identify underlying patterns in the data, and reduce the dimensionality of the data. In many instances data can have non linear trends with regard to the response or predictor variables. Additionally, data can also have non-normal distributions. This situation can be problematic for many modeling techniques. To address this situation, analyst will attempt to transform their data using one of many common [transformations](https://scikit-learn.org/1.5/data_transforms.html). Similarly, ordination attempts to project and graphically display complex relationships among predictor variables. Additionally, it can be used to reduce the dimensionality of data and or project the data to highlight various aspects of the data. \n",
    "\n",
    "Using our scaled data we will explore common transformations, demonstrate a ordination technique called principal component analysis [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca), and evaluate how much information is in each components. First let's summarize and look at our data to see if there is any underlying trends in our scaled predictors with regards to our response variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot matrix of continuous predictors and tree canopy cover colored by tree density class\n",
    "col={'low':'blue','medium':'green','high':'red','non-forested':'orange'}\n",
    "gdf_m3_sc=gdf_m3[['sc_ls_1','sc_ls_2','sc_ls_3','sc_ls_4','sc_ls_5','sc_ls_6','sc_ls_7','sc_el_1','tcc_1']]\n",
    "display(gdf_m3_sc.describe())\n",
    "pd.plotting.scatter_matrix(gdf_m3_sc,color=gdf_m3['forest_density'].replace(col),figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot matrix graphically display a lot of information. If we look at the bottom row (tcc_1), we can visually see any underlying trends between TCC and each of the scaled landsat and elevation variables. Looking at the other rows of our matrix, let us see underlying relationships or trends among predictor variables. Finally, looking at the diagonal of the matrix reveals the underlying distribution of each variables. \n",
    "\n",
    "What are you first impressions of the data?\n",
    "    - Do you see any trending between TCC and the predictor variables?\n",
    "    - Do you see any clumping between Forest Density (the colors) and the predictor variables?\n",
    "    - Are all the variables normally distributed?\n",
    "    - Are there any trends among predictor variables?\n",
    "\n",
    "For example, let's look specifically at the tcc_1 histogram. The distribution of tcc-1 appears to be skewed left. To make this data more normally distributed, we can square or cube our tcc_1 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cubing tcc_1\n",
    "\n",
    "vls=gdf_m3_sc['tcc_1']**3\n",
    "vls.plot(kind='hist')\n",
    "\n",
    "#does this make tcc_1 more normal?\n",
    "#What impact does this have with regard to the predictor variable?\n",
    "#Could we make a new scatter plot with the transformed value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the scatter plot matrix can be used to identify trends in predictor variables in a multivariate way. Looking back at the scatter plot matrix we can see that there is significant linear trends in the predictor variables. This can be problematic for some modeling techniques. To address the colinear issue we can use a ordination technique like principal component analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn PCA decomposition module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#create a PCA object\n",
    "pca=PCA()\n",
    "\n",
    "#fit the scaled Landsat data (sdf1)\n",
    "pca.fit(sdf1)\n",
    "\n",
    "#Look at the loading values components\n",
    "display(pd.DataFrame(pca.components_,columns=pca.get_feature_names_out()))\n",
    "\n",
    "#Look at the % variance explained by each component\n",
    "display(pd.DataFrame(pca.explained_variance_ratio_,index=pca.get_feature_names_out(),columns=['e_var']).plot(kind='bar',use_index=True))\n",
    "print('% variance explained\\n',pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of transformed values\n",
    "pca_df=pd.DataFrame(pca.transform(sdf1),columns=pca.get_feature_names_out())\n",
    "\n",
    "#Add those values back to our gdf_m3 dataframe\n",
    "gdf_m3=pd.concat([gdf_m3,pca_df],axis=1)\n",
    "\n",
    "#Look at the new scatter plot of values including the transformed tcc variable\n",
    "gdf_m3['t_tcc']=vls\n",
    "pd.plotting.scatter_matrix(gdf_m3[['t_tcc','pca0','pca1','pca2','pca3','pca4','pca5','pca6','pca7']],color=gdf_m3['forest_density'].replace(col),figsize=(15,15))\n",
    "\n",
    "\n",
    "#What pca columns should we keep?\n",
    "#Should we standardize our Landsat values or scale them?\n",
    "#What do the PCA loadings mean?\n",
    "#what is another ordination technique we could use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Looking for patterns in the data\n",
    "Often patterns are easier to see in data that have been transformed. We can see that there appears to be somewhat of a linear trend between t_tcc and pca0 and pca1. Let's look closer at this relationship by creating a scatter plot of t_tcc by pca0 and pca1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot of pca0 by pca1 with t_tcc as a color\n",
    "print('PCA 0 by PCA 1 with t_tcc colored green as t_tcc increases')\n",
    "gdf_m3.plot(kind='scatter',x='pca0',y='pca1',cmap='BuGn',color=gdf_m3['t_tcc'],figsize=(15,15))\n",
    "\n",
    "# What would this look like if we used forest density?\n",
    "#gdf_m3.plot(kind='scatter',x='pca0',y='pca1',color=gdf_m3['forest_density'].replace(col),figsize=(15,15))\n",
    "# Are low, medium, and high densities grouped?\n",
    "# Why did we just plot PCA0 by PCA1?\n",
    "# What proportion of the information does PCA0 and PCA1 account for in the data?\n",
    "# What other techniques could we use to transform our data? \n",
    "# Which variables should we use in our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKVSiigPdFaF"
   },
   "source": [
    "### Section 7: Building a predictive model\n",
    "#### In this section we will explore how to build a model that can be used later to predict new estimates. While there are many different modeling techniques that can be used to group data or draw a relationship between response and predictor variables, we can generally split these techniques into three groups: 1) clustering, 2) regression, and 3) classification. \n",
    "\n",
    "- Clustering is used to group observations with similar values and can include continuous and categorical variables depending on the technique.\n",
    "- Regression is used to estimate a continuous response value. Predictor variables can be either continuous or categorical.\n",
    "- Classification is used to estimate a categorical response value. Predictors can be either continuous or categorical.\n",
    "\n",
    "To demonstrate each instance, we will use the following techniques to group like observations, estimate transformed tcc, and classify tree density:\n",
    "- K-means for clustering\n",
    "- Random Forest for regression \n",
    "- Nnet regression for a classification\n",
    "\n",
    "While there are many different modeling techniques one can use, it is very important to understand the underlying assumptions of a given model and when one modeling technique would be preferred to another technique. For a full description of each modeling technique please refer to the scientific literature. \n",
    "\n",
    "#### Key learning points:\n",
    "- Training, testing, and validation\n",
    "- Modeling and inference\n",
    "- Saving a model\n",
    "\n",
    "#### Coding steps\n",
    "1. Training, testing, and validation\n",
    "2. Model inference\n",
    "3. Saving a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Training, testing, and validation\n",
    "Many of the machine learning techniques do an excellent job at fitting the data exactly. However this situation can lead to a model that does very well reproducing the data but that does not generalize well to new data (over fitting). To evaluate how well a model is doing for new observations, data scientist often employ a technique that splits data into multiple groups and uses one group to train the model (training data), another group to evaluate the model (test data), and even a third group, separate from what was used to train or test the model, to evaluate the model. In this next exercise we will subset our gdf_m3 dataframe into response and predictor variables and further split our dataset into 3 groups that will be used to training (~33%), testing (~33%), and validation (~33%) our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the data to the variables we will be using in the models\n",
    "sub_gdf=gdf_m3[['forest_density','red_cat_1.0', 'red_cat_2.0', 'red_cat_3.0', 'pca0', 'pca1', 'pca2',\n",
    "       'pca3', 'pca4', 'pca5', 'pca6','pca7', 't_tcc']].copy()\n",
    "\n",
    "#get the total number of observation in the dataframe\n",
    "n=sub_gdf.shape[0]\n",
    "\n",
    "#create a random number for each observation in the dataframe\n",
    "sub_gdf.loc[:,'rnd']=np.random.random(n)\n",
    "\n",
    "#sort the dataframe by the random numbers\n",
    "sub_gdf=sub_gdf.sort_values('rnd')\n",
    "\n",
    "#split the data into 3 groups based on their sorted row count\n",
    "sp=int(n*0.33)\n",
    "train=sub_gdf.iloc[:sp]\n",
    "test=sub_gdf.iloc[sp:sp*2]\n",
    "val=sub_gdf.iloc[sp*2:]\n",
    "\n",
    "#look at the training data\n",
    "train\n",
    "\n",
    "#How many observations are in the train, test, and val?\n",
    "#Can you think of a different way to randomly select observations from your dataframe?\n",
    "#Should the distribution of each dataframe be the same?\n",
    "#Are the distribution of each dataframe the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Modelling and inference\n",
    "Now that we have 3 datasets we can begin building models and interpreting the results. In the next few cells we will do the following:\n",
    "- build a k-means clustering model, test our estimates, validate our model, and interpret our results\n",
    "- build a Random Forest regression model, test our estimates, validate our model, and interpret our results\n",
    "- build a Multinomial logistic regression model for classification test our estimates, validate our model, and interpret our results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means cluster example using PCA variables. Clustering is typically used to group like observations. To evaluate our clusters in this case we will compare the distribution of our cluster labels for our training, testing, and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# create the X data frame of predictors (only continuous variable are applicable for kmeans)\n",
    "pred_clm=['pca0', 'pca1', 'pca2',\n",
    "       'pca3', 'pca4', 'pca5', 'pca6','pca7']\n",
    "X_train=train[pred_clm]\n",
    "\n",
    "X_test=train[pred_clm]\n",
    "\n",
    "X_val=val[pred_clm]\n",
    "\n",
    "# create the KMeans object with 10 clusters\n",
    "km=KMeans(10)\n",
    "\n",
    "#fit the model using the training data\n",
    "km.fit(X_train)\n",
    "\n",
    "#apply the model to train, test, and validation datasets\n",
    "train['lbl']=km.predict(X_train)\n",
    "test['lbl']=km.predict(X_test)\n",
    "val['lbl']=km.predict(X_val)\n",
    "\n",
    "#look at the box plot of each lbl and principal component value\n",
    "train[pred_clm+['lbl']].plot(kind='box',by='lbl',figsize=(15,4))\n",
    "test[pred_clm+['lbl']].plot(kind='box',by='lbl',figsize=(15,4))\n",
    "val[pred_clm+['lbl']].plot(kind='box',by='lbl',figsize=(15,4))\n",
    "\n",
    "#Do the distribution look different for each dataset?\n",
    "#What are the means and standard deviation for each label within each dataset?\n",
    "#Can we test to see if the distributions are different?\n",
    "\n",
    "#How can we see if each class label is roughly the same size?\n",
    "# display(np.unique(train['lbl'],return_counts=True))\n",
    "# display(np.unique(test['lbl'],return_counts=True))\n",
    "# display(np.unique(val['lbl'],return_counts=True))\n",
    "\n",
    "\n",
    "#Which label has the smallest number of observations?\n",
    "#What would happen if you selected 5 clusters?\n",
    "#Did we really need a train, test, and validation dataset?\n",
    "\n",
    "#Create a scatter plot of pca0 by pca1 with observations colored based on the k-mean label\n",
    "# train.plot(kind='scatter',x='pca0',y='pca1',color=train['lbl'],cmap='Paired',figsize=(15,15))\n",
    "\n",
    "#why would we cluster objects?\n",
    "#What other clustering techniques are available?\n",
    "#How could we use our categorical values?\n",
    "#Is there a relationship between our clusters and forest_density?\n",
    "#Is there a relationship between our clusters and TCC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating Tree Canopy Cover (TCC) using Random Forest Regression and PCA and red_cat predictor variables. Random Forest Regression is an ensemble of CART models which can be used to estimate continuous variables (it can also be used for classification purposes). It can use both continuous and categorical predictor variables as inputs. To evaluate or model, we will apply it on test and validation datasets and compare the observed TCC values against the Predicted TCC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Create our predictor and response arrays\n",
    "pred=['red_cat_1.0', 'red_cat_2.0', 'red_cat_3.0', 'pca0',\n",
    "       'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6','pca7']\n",
    "resp='t_tcc'\n",
    "\n",
    "\n",
    "Xtr=train[pred]\n",
    "ytr=train[resp]\n",
    "\n",
    "#get our test and validation array\n",
    "Xts=test[pred]\n",
    "Xvl=val[pred]\n",
    "\n",
    "#create the Random forest regressor \n",
    "rndf = RandomForestRegressor(oob_score=True,max_samples=0.8, random_state=0)\n",
    "\n",
    "#fit the model\n",
    "rndf.fit(Xtr, ytr)\n",
    "\n",
    "#Fit stats\n",
    "print('OOB r-squared =',rndf.oob_score_)# out of bag r squared (% of variance explained)\n",
    "\n",
    "#plot variable importance\n",
    "pd.DataFrame(rndf.feature_importances_,index=pred,columns=['importance']).sort_values(by='importance').plot(kind='barh', title='Variable Importance')\n",
    "\n",
    "#predict our values for test and validation\n",
    "test.loc[:,'pred_tcc']=rndf.predict(Xts)\n",
    "val.loc[:,'pred_tcc']=rndf.predict(Xvl)\n",
    "\n",
    "#display r-squared and plot observed vs predicted for test\n",
    "print('test r-squared =',test[['t_tcc','pred_tcc']].corr().iloc[0,1]**2)\n",
    "print('test RMSE =', np.sqrt(((test['t_tcc']-test['pred_tcc'])**2).mean()))\n",
    "test.plot(kind='scatter',y='t_tcc',x='pred_tcc',title='Test obs vs pred')\n",
    "\n",
    "#display r-squared and plot observed vs predicted for test\n",
    "print('val r-squared =',val[['t_tcc','pred_tcc']].corr().iloc[0,1]**2)\n",
    "print('val RMSE =', np.sqrt(((val['t_tcc']-val['pred_tcc'])**2).mean()))\n",
    "val.plot(kind='scatter',y='t_tcc',x='pred_tcc',title='Val obs vs pred')\n",
    "\n",
    "#which variables are most important?\n",
    "#How is importance being calculated?\n",
    "#What units are t_tcc?\n",
    "#if we want to look at TCC what do we need to do?\n",
    "# print('TCC r-squred =',(val[['t_tcc','pred_tcc']]**(1/3)).corr().iloc[0,1]**2)\n",
    "# (val[['t_tcc','pred_tcc']]**(1/3)).plot(kind='scatter',y='t_tcc',x='pred_tcc')\n",
    "#Does our obb give a reliable estimate of r-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying forest_density using a neural network (Nnet) and PCA and red_cat predictor variables. Like Random Forest, Nnets can be used for regression and classification. Additionally, Nnets can use both continuous and categorical predictor variables as inputs. To evaluate or model, we will apply it on test and validation datasets and compare the observed forest_density classes against the predicted classes values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "\n",
    "nnet_cl=MLPClassifier(random_state=0,activation='logistic',max_iter=300)\n",
    "\n",
    "pred=['red_cat_1.0', 'red_cat_2.0', 'red_cat_3.0', 'pca0',\n",
    "       'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6','pca7']\n",
    "resp='forest_density'\n",
    "\n",
    "#get our train data array\n",
    "Xtr=train[pred]\n",
    "ytr=train[resp]\n",
    "\n",
    "#get our test and validation array\n",
    "Xts=test[pred]\n",
    "Xvl=val[pred]\n",
    "\n",
    "#fit the model\n",
    "nnet_cl.fit(Xtr,ytr)\n",
    "\n",
    "\n",
    "#Fit stats\n",
    "print('Nnet overall accuracy (training) =',accuracy_score(y_true=ytr, y_pred=nnet_cl.predict(Xtr)))# % accuracy\n",
    "print('Nnet overall accuracy(test) =',accuracy_score(y_true=test[resp], y_pred=nnet_cl.predict(Xts)))# % accuracy\n",
    "print('Nnet overall accuracy(validation) =',accuracy_score(y_true=val[resp], y_pred=nnet_cl.predict(Xvl)))# % accuracy\n",
    "print('\\nNnet Contingency_matrix (validation):\\n',contingency_matrix(labels_true=val[resp],labels_pred=nnet_cl.predict(Xvl)))\n",
    "\n",
    "#Calculate the class probabilities for train, test and val\n",
    "tr_prob=pd.DataFrame(nnet_cl.predict_proba(Xtr),columns=nnet_cl.classes_)\n",
    "ts_prob=pd.DataFrame(nnet_cl.predict_proba(Xts),columns=nnet_cl.classes_)\n",
    "vl_prob=pd.DataFrame(nnet_cl.predict_proba(Xvl),columns=nnet_cl.classes_)\n",
    "\n",
    "#What does the overall accuracy and the contingency matrix for train, test and val tell us?\n",
    "#What do these probabilities tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scatter plot of probabilities vs pca0 holding other predictors at the 25th quantiles\n",
    "dsc=Xtr.describe()\n",
    "dsc_mean=dsc.loc['25%'].drop(index=['pca0'])\n",
    "dsc_min=dsc.loc['min']\n",
    "dsc_max=dsc.loc['max']\n",
    "stp=(dsc_max.loc['pca0']-dsc_min.loc['pca0'])/100\n",
    "pca0=np.arange(dsc_min.loc['pca0'],dsc_max.loc['pca0'],stp)\n",
    "df_graph=pd.DataFrame(pca0,columns=['pca0'])\n",
    "df_graph.loc[:,dsc_mean.index.values]=dsc_mean.values\n",
    "df_graph[nnet_cl.classes_]=nnet_cl.predict_proba(df_graph[Xtr.columns])\n",
    "color={'high':'red','medium':'blue','low':'green','non-forested':'orange'}\n",
    "p=df_graph.plot(x='pca0',y=nnet_cl.classes_[0],color=color[nnet_cl.classes_[0]],figsize=(15,8))\n",
    "for i in nnet_cl.classes_[1:]:\n",
    "  p=df_graph.plot(ax=p,x='pca0',y=i,color=color[i],figsize=(15,8))\n",
    "\n",
    "p\n",
    "\n",
    "#What does this graphic tell us?\n",
    "#how would you calculate probability error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Saving the model\n",
    "Often we will want to store a given model to use at a later time. We can easily serialize and load a model using the pickle package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#let's store our kmeans, random forest, and nnet models\n",
    "pickle.dump(km,open('km.mdl','wb'))#kmeans\n",
    "pickle.dump(rndf ,open('rndf.mdl','wb'))#random forest regression\n",
    "pickle.dump(nnet_cl,open('nnet_cl.mdl','wb'))#neural network classification\n",
    "\n",
    "#to load our model back into ram we can call the load method for each saved model\n",
    "km = pickle.load(open('km.mdl', 'rb'))\n",
    "rndf = pickle.load(open('rndf.mdl', 'rb'))\n",
    "nnet_cl = pickle.load(open('nnet_cl.mdl', 'rb'))\n",
    "\n",
    "#let's look at the oob r-squared value from our loaded random forest regression model\n",
    "rndf.oob_score_\n",
    "\n",
    "#How can you use your stored model?\n",
    "#Why would you want to store a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBQ3ZycSdFaF"
   },
   "source": [
    "### Section 8: Clustering and estimating % Tree Canopy Cover and Forest Density Classes\n",
    "#### In this section we will explore how to use a model to create new estimates using Raster Tools and our 2016 Landsat scene.\n",
    "\n",
    "#### Key learning points:\n",
    "- What are we estimating?\n",
    "- Estimation domain\n",
    "- Estimates of error\n",
    "\n",
    "#### Coding steps\n",
    "1. Transforming raster surfaces to standardized values\n",
    "2. Applying a model to raster surfaces\n",
    "3. Interpolating results (no extrapolation)\n",
    "4. Estimating error at the cell level\n",
    "5. Estimating error at a coarser scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Transforming Raster surfaces to standardized PCA and encoded values\n",
    "In order to build our models we need to go back through the steps used to scale and transform our data. Fortunately, we still have our MinMax scaler, and PCA models which we can use to create a function that can convert landsat cell values into values our models are expecting. Likewise, we can use the functionality within Raster Tools to duplicate our OneHoteEconding model. In total, we have 11 predictor variables (8 principal components and 3 OneHotEncoded variables). This means that we need to create a 11 band raster surface with values that each of our models are expecting (band order being the same as what was used to create the model).\n",
    "- km model - pca0-pca7\n",
    "- rndf model = pca0-pca7 and red_cat1.0-3.0 with value of 1 or 0\n",
    "- nnet_cl model = pca0-pca7 and red_cat1.0-3.0 with values of 1 or 0\n",
    "\n",
    "Let's start with creating our PCA surfaces, then create our encoded surfaces, and finally combine our surfaces for each modeling technique. To accomplish this we will use multiple Raster Tools functions. Of special interest are the [ModelPredictAdaptor](https://um-rmrs.github.io/raster_tools/reference/generated/raster_tools.general.ModelPredictAdaptor.html#raster_tools.general.ModelPredictAdaptor) class and [model_predict](https://um-rmrs.github.io/raster_tools/reference/generated/raster_tools.general.model_predict_raster.html#raster_tools.general.model_predict_raster) functions. \n",
    "- The ModelPRedictAdaptor class allows users to assign a \"predict\" function from a class object that can be used to predict new values. This function can be any function that takes an array of values and returns an array of values in the same manner as sklearn model predict functions. \n",
    "- The model_predict function uses a specified class object predict function to estimate new observation given the raster's cell values. Note, this same process can be used for vector datasets and will return a dataframe of estimated values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Let's use a pipeline to combine the scaling and PCA steps\n",
    "pipe = Pipeline(steps=[(\"scaler\", mms), (\"pca\", pca)])\n",
    "\n",
    "#first we need to snap our raster datasets to the same cell location\n",
    "dem2=dem.reproject(ls82016.geobox)\n",
    "\n",
    "#Let's stack our landsat and dem raster surfaces\n",
    "ors=general.band_concat([ls82016,dem2])\n",
    "\n",
    "#Let's use Raster Tools ModelPredictAdaptor to create a model that has predict function that Raster Tools can use\n",
    "pred_tr=general.ModelPredictAdaptor(pipe,'transform')\n",
    "\n",
    "#Apply that model to our stacked raster to create input data for our kmeans model\n",
    "pred_pca_rs=ors.model_predict(pred_tr,len(pca.get_feature_names_out()))\n",
    "\n",
    "#Let's use logical functions to create our red_cat variables from landsat band 1\n",
    "ls1_rs=ls82016.get_bands(1)\n",
    "d_rs=(ls1_rs<0.022).astype('int')\n",
    "b_rs=(ls1_rs>0.024).astype('int')\n",
    "n_rs=((0.020<=ls1_rs) & (ls1_rs<=0.024)).astype('int')\n",
    "\n",
    "#Let's combine our raster surfaces to create a 11 band raster we can use with our random forest and nnet models. \n",
    "#Make sure the order of the surfaces matches the order used to create the model\n",
    "pred_rs=general.band_concat([d_rs,n_rs,b_rs,pred_pca_rs])\n",
    "\n",
    "#what does our pca raster look like?\n",
    "# import xarray as xr\n",
    "# rs_lst=[]\n",
    "# for b in range(3):\n",
    "#     rs=pred_pca_rs.get_bands(b+1)\n",
    "#     bmin=rs.min()\n",
    "#     bmax=rs.max()\n",
    "#     rs_lst.append((rs-bmin)/(bmax-bmin))\n",
    "\n",
    "# rgb=general.band_concat(rs_lst).xdata\n",
    "# rgb=rgb.where(rgb>0,np.nan)\n",
    "# xr.plot.imshow(rgb,figsize=(15,8),robust=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Applying a model to raster surfaces\n",
    "Now that we have our predictor surfaces, we can apply our km, rndf, and nnet_cl models using Raster Tools model_predict function. Let's start with our K-means model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's perform our kmeans clustering algorithm and visualize the results\n",
    "import folium\n",
    "\n",
    "#create a folium map\n",
    "m=folium.Map()\n",
    "\n",
    "#create our raster surface of our km classes\n",
    "km_rs=pred_pca_rs.model_predict(km,1) #what does the one mean?\n",
    "\n",
    "#add the NAIP imagery as a backdrop\n",
    "folium.TileLayer(\n",
    "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr = 'Esri',\n",
    "        name = 'Esri Imagery',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(m)\n",
    "\n",
    "#Let's add our training plot locations to the map\n",
    "m=gdf_m3.loc[train.index].explore(m=m,color='black',name='training')\n",
    "\n",
    "#add our kmeans raster to the map\n",
    "m=km_rs.explore(map=m,band=1,cmap='tab10', name='kmean classes')\n",
    "\n",
    "#display the map\n",
    "m\n",
    "\n",
    "#what do the colors mean?\n",
    "# Did we need to standardize our predictors?\n",
    "# Did we need to run the PCA analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate TCC using our random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a folium map\n",
    "m=folium.Map()\n",
    "\n",
    "#create our raster surface of TCC\n",
    "tcc_rs=pred_rs.model_predict(rndf,1)**(1/3) #why did we take the cuberoot of the rndf estimate?\n",
    "\n",
    "#add the NAIP imagery as a backdrop\n",
    "folium.TileLayer(\n",
    "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr = 'Esri',\n",
    "        name = 'Esri Imagery',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(m)\n",
    "\n",
    "#Let's add our training plot locations to the map\n",
    "m=gdf_m3.loc[train.index].explore(m=m,color='black',name='training')\n",
    "\n",
    "#add our TCC raster to the map\n",
    "m=tcc_rs.explore(map=m,band=1,cmap='PRGn', name='TCC %')\n",
    "\n",
    "#display the map\n",
    "m\n",
    "\n",
    "#what do the colors mean?\n",
    "# Did we need to standardize our predictors?\n",
    "# Did we need to run the PCA analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate density class using our Nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a folium map\n",
    "m=folium.Map()\n",
    "\n",
    "#create our raster surface of class probabilities for later use\n",
    "pred_prob=general.ModelPredictAdaptor(nnet_cl,'predict_proba')\n",
    "prob_rs=pred_rs.model_predict(pred_prob,len(nnet_cl.classes_))\n",
    "\n",
    "#create a definition to convert character values to numbers \n",
    "def nn_cl_lb(X):\n",
    "    X2=nnet_cl.predict(X)\n",
    "    outarr=np.zeros_like(X2,dtype='int32')\n",
    "    outarr[X2=='low']=1\n",
    "    outarr[X2=='medium']=2\n",
    "    outarr[X2=='high']=3\n",
    "    return outarr\n",
    "\n",
    "#add the function to our nnet_cl object\n",
    "nnet_cl.nn_cl_lb=nn_cl_lb\n",
    "\n",
    "#use teh ModelPredictAdaptor to specify our funtion\n",
    "nnet_cl2=general.ModelPredictAdaptor(nnet_cl,'nn_cl_lb')\n",
    "\n",
    "#create a folium map\n",
    "m=folium.Map()\n",
    "\n",
    "#create our raster surface of class labels 0-3\n",
    "dens_rs=pred_rs.model_predict(nnet_cl2,1)\n",
    "\n",
    "#print out the coded values\n",
    "print('non-forested=0','low=1','medium=2','high=3')\n",
    "\n",
    "#add the NAIP imagery as a backdrop\n",
    "folium.TileLayer(\n",
    "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr = 'Esri',\n",
    "        name = 'Esri Imagery',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "       ).add_to(m)\n",
    "\n",
    "#Let's add our training plot locations to the map\n",
    "m=gdf_m3.loc[train.index].explore(m=m,color='red',name='training')\n",
    "\n",
    "#add our dens raster to the map\n",
    "m=dens_rs.explore(map=m,band=1,cmap='YlGn', name='Tree Density Class (yellow-green 0-3)')\n",
    "\n",
    "#display the map\n",
    "m\n",
    "\n",
    "# Did we need to standardize our predictors?\n",
    "# Did we need to run the PCA analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Interpolating results (no extrapolation)\n",
    "\n",
    "One import issue that is often overlooked when applying models to new observations is determining when a model is interpolating vs extrapolating. So what does this mean? First, let's define these two cases. Interpolating means that predictions are made for new observations in which the predictor values fall within the range of values used to train the model. While, extrapolation occurs when the predictor values used to estimate the new observation fall outside of the range used to calibrate the model. In all cases interpolated estimates are preferred to extrapolated estimate. \n",
    "\n",
    "This issue is also closely related to the number of variables used to train a model and the model's ability to be generalized to the population. Generally, this means that as you increase the number of variables used in a model, you also need to increase the number of observation used to train the model to insure capturing the range of all potential predictor variable values (domain). This situation is often referred to as the curse of dimensionality. Can you think of other issues associated with increasing the number of predictor variables used in a model? \n",
    "\n",
    "When designing a sample or training set, special effort should be made to insure your sample is well spread and balanced to the naturally occurring distribution of the population. Additionally, once a model has been built it is always a good idea to define the range in which estimates are being interpolated vs extrapolating. To do this let's assume we will be using all predictor variables. Using our training dataframe we can build a simple model that queries all cells within our predictor image that fall within the range of sampled values using the values returned from describing our data. This new raster surface can then be used as a mask that spatially describes where the model is interpolating values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raster_tools import creation\n",
    "\n",
    "#describe the training data\n",
    "dsc=train.describe().iloc[:,:11]\n",
    "\n",
    "#get the min and max values for each predictor column\n",
    "min_dsc=dsc.loc['min']\n",
    "max_dsc=dsc.loc['max']\n",
    "\n",
    "#create a msk raster with values of 0\n",
    "msk=creation.full_like(pred_rs,0)\n",
    "\n",
    "#iterate through each raster band and select cells that fall within the sampled range\n",
    "for b in pred_rs.band:\n",
    "    rs=pred_rs.get_bands(b)\n",
    "    min_vl=min_dsc.iloc[b-1]\n",
    "    max_vl=max_dsc.iloc[b-1]\n",
    "    ch=(rs >= min_vl) & (rs <= max_vl)\n",
    "    msk=msk + ch.astype('int32') #add all cells falling within the min and max to the mask raster\n",
    "\n",
    "#select only the cells that fall within the sampled range (sum to 11)\n",
    "msk=(msk==dsc.shape[1]).astype('int32')\n",
    "\n",
    "#use mask to mask km, tcc, and density class values\n",
    "km_rs_m=km_rs.where(msk,np.nan)\n",
    "tcc_rs_m=tcc_rs.where(msk,np.nan)\n",
    "dens_rs_m=dens_rs.where(msk,np.nan)\n",
    "\n",
    "#visualize TCC\n",
    "tcc_rs_m.explore(band=1,cmap='PRGn', name='TCC %')\n",
    "#km_rs_m.explore(band=1,cmap='tab10', name='K-mean Classes')\n",
    "#dens_rs_m.explore(band=1,cmap='YlGn', name='Density Classes')\n",
    "\n",
    "#What percentage of the pixels in our image fall within our sampled range?\n",
    "#What percentage of the pixels in our image fall within the sampled range of PCA0-PCA3?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Estimating error at the cell level\n",
    "So far we have created multiple models using our training sample, evaluated the overall accuracy and fit of our models (fit statistics), applied our models back to our imagery, and have determined a spatial mask that defines where (which cells) we are interpolating estimates as opposed to extrapolating estimates. However, we have not directly discussed how to spatially describe estimation error. While this has less applicability for our clustering model (each cluster has a mean and the variation around that mean (distance) is used to assign observations to a given cluster), for regression and classification models, estimation error is important to describe. That is, each estimate we make for each cell in our image is a mean estimate. That mean estimate has variation which we can use to calculate the standard error of that estimate and/or a confidence interval. To calculate standard error for each estimate we can employee two broad techniques; 1) use classical statistical distributions to estimate standard error or 2) empirically derive estimates of standard error using bootstrapping. To demonstrate how to calculate standard error for our TCC regression and our forest density models we will employee a bootstrapping and classical distribution approach, respectively.\n",
    "\n",
    "To begin with, let's look at the TCC regression model. Remember that this model is an ensemble of CART models and that each CART model is derived from a random subset of our training data that selects observations at random with replacement. For new observations, each CART model is used to estimate a mean for each observation in our new dataframe of observations. These mean values are then averaged to determine a overall mean value for that observation. This overall mean value is the estimate applied to each raster cell in our previous TCC image. To calculate standard error for each image cell using bootstrapping, we simply need to calculate the standard deviation of each CART model estimate for that cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's build a definition that calculates standard error from our rndf model\n",
    "def standard_error(X):\n",
    "    ss=np.zeros(X.shape[0])\n",
    "    ss2=np.zeros(X.shape[0])\n",
    "    N=rndf.n_estimators\n",
    "    #iterate through each CART model and estimate the mean value. Calculate standard deviation of mean estimates one pass\n",
    "    for mdl in rndf.estimators_:\n",
    "        est=mdl.predict(X)**(1/3) #we need to take the cube root to address the cubing TCC\n",
    "        ss = ss+est\n",
    "        ss2=ss2+est**2\n",
    "    \n",
    "    return np.sqrt((ss2-(ss**2/N))/N) #population estimate\n",
    "\n",
    "#add our definition to our rndf object\n",
    "rndf.standard_error =standard_error\n",
    "\n",
    "#create our model predict adaptor\n",
    "rndf_se=general.ModelPredictAdaptor(rndf,'standard_error')\n",
    "\n",
    "#create our standard error raster using the model_predict fucntion\n",
    "tcc_se_rs=pred_rs.model_predict(rndf_se,1)\n",
    "\n",
    "#visualize the results\n",
    "tcc_se_rs.explore(band=1,cmap='PRGn', name='TCC (%) Standard Error')\n",
    "\n",
    "#How many CART models make up our random forest model?\n",
    "#what does our standard error map show us?\n",
    "#What is the maximum standard error? Does that change if we use the mask?\n",
    "#what is the minimum standard error? Does that change if we use the mask?\n",
    "#how could you convert standard error to a 95% confidence interval?\n",
    "#are there spatial trend in standard error? If so, is that a problem?\n",
    "#what estimates have teh largest standard errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate standard error for Nnet classification of forest density. In this case we only have one Nnet. We could employee a similar strategy as with the random forest model if we used bagging and developed many Nnets for subsets of the training data but that would require making a new type of of model, a ensemble of Nnets. Instead, let's assume a multinomial distribution of our data and use basic statistical properties. To do this we need to recognize that our classification is really a rule based on the probability that each class is present at each cell, given our predictor variable input. The rule used to turn probabilities to a given label is very simple. The class with the largest probability get the corresponding label. In this case we really are not as interested with the rule as much as the probabilities and the error associated with the class probabilities. That means we will be estimating standard error for each class probability estimate. For a multinomial distribution, standard error can be approximated as a function of the probability:\n",
    "\n",
    "$se = \\sqrt{p(1-p)/N}$\n",
    "\n",
    "Where p = the probability at each cell and N = the Train sample size. Let's use this formula to estimate the standard error for each class probability at each raster cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training sample size\n",
    "N=train.shape[0]\n",
    "\n",
    "#calculate the se for each band cell\n",
    "prob_rs_se=(prob_rs*(1-prob_rs)/N)**0.5\n",
    "\n",
    "#visualize the standard error for high density class probability\n",
    "#prob_rs.explore(band=1,cmap='PRGn', name='Probability of High Forest Density') #visualize the probability surface for the high density class\n",
    "prob_rs_se.explore(band=1,cmap='PRGn', name='Probability of High Forest Density Standard Error')\n",
    "\n",
    "#What units are we working with?\n",
    "#what does our standard error map show us?\n",
    "#What is the maximum standard error? Does that change if we use the mask?\n",
    "#what is the minimum standard error? Does that change if we use the mask?\n",
    "#how could you convert standard error to a 95% confidence interval?\n",
    "#are there spatial trend in standard error? If so, is that a problem?\n",
    "#what probabilities have the largest standard errors?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a TCC and Forest Density Class surface and various estimates of error and accuracy derived from our training data. Additionally, we have the original TCC and Forest Density surfaces for 2016. Let's see how different the estimates are at the cell level.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's subtract the original TCC from our Estimate of TCC for the domain of our model (mask)\n",
    "tcc_dif=tcc2016.reproject(tcc_rs.geobox)-tcc_rs\n",
    "tcc_dif=tcc_dif.load()\n",
    "tcc_dif=tcc_dif.where(msk,np.nan)\n",
    "\n",
    "N=(tcc_dif**2 > 0).sum().compute()\n",
    "rmse=np.sqrt((tcc_dif**2).sum().compute()/N)\n",
    "\n",
    "print('min dif =', tcc_dif.min().compute(),'max dif =',tcc_dif.max().compute(),'RMSE =',rmse,'Mean dif =',tcc_dif.mean().compute())\n",
    "print('mean TCC 2016 =',tcc2016.reproject(tcc_rs.geobox).where(msk,np.nan).mean().compute(), 'mean estimated TCC 2016 =',tcc_rs.where(msk,np.nan).mean().compute())\n",
    "\n",
    "tcc_dif.explore(band=1)\n",
    "\n",
    "#How would our estimates change if we removed the mask?\n",
    "#How would our estimates change if we subset to the boundary of the forest?\n",
    "#What was the RMSE from our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's convert the original TCC to forest density classes and see how many times the cells match\n",
    "dens_rs2016=tcc2016.reproject(tcc_rs.geobox).remap_range(((0,1,0),(1,20,1),(20,81,2),(81,10000,3))).astype('u8')\n",
    "ch=(dens_rs2016==dens_rs).where(msk,np.nan)\n",
    "print(np.unique(ch,return_counts=True))\n",
    "\n",
    "#why are there so many incorrect labels (0 values)?\n",
    "#what was our overall map accuracy?\n",
    "#What is our map accuracy for these cells?\n",
    "\n",
    "#can you create a contingency table and track the differences?\n",
    "# tvls=dens_rs2016.values.flatten()\n",
    "# pvls=dens_rs.astype('u8').values.flatten()\n",
    "# msk2=msk.values.flatten()\n",
    "# print('contingency table (cells)')\n",
    "# ci=['non-forested','low','medium','high']\n",
    "# ci2=['non-forested','medium','high']\n",
    "# display(pd.DataFrame(contingency_matrix(labels_true=tvls[(msk2==1) & ((tvls<4)&(pvls<4))],labels_pred=pvls[(msk2==1) & ((tvls<4)&(pvls<4))]),index=ci,columns=ci2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Estimating error at a coarser scale \n",
    "In the previous section we looked at estimates at the cell level and even some estimates at the model domain level. When comparing our estimates at the cell level we simply compared mean values. We could have also included estimates based on se. In this step we are going to look at how we can generate estimates for larger domains than the cell but smaller than the entire modeling domain. To do that we first need to answer a few questions. \n",
    "- What is the domain of our sample?\n",
    "- What is the domain of our models?\n",
    "- What are we trying to estimate, the cell, the forest polygon, only high density forest cells?\n",
    "\n",
    "First, let's estimate the average TCC for the national forest using our sample and design based inference. To evaluate our estimate, we will compare our mean estimate to the actual mean of cells within the national forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from raster_tools import clipping\n",
    "\n",
    "#clip our tcc2016 image to the boundary of the national forest\n",
    "msk = clipping.mask(nfp,tcc2016).to_null_mask()\n",
    "cl_tcc2016=tcc2016.where(msk==0,np.nan)\n",
    "\n",
    "#calculate the cell mean\n",
    "p_mean=cl_tcc2016.mean().compute()\n",
    "\n",
    "#calculate the training sample mean and se\n",
    "\n",
    "tcc_vls=train['t_tcc']**(1/3)\n",
    "n=tcc_vls.shape[0]\n",
    "tvl=scipy.stats.t.ppf(0.975,n-1) #t-inverse used to estimate confidence intervals\n",
    "s_mean=tcc_vls.mean()\n",
    "s_se=tcc_vls.std()/np.sqrt(n)\n",
    "\n",
    "#print actual values\n",
    "print('True mean =',p_mean)\n",
    "print('Sample se =',s_se)\n",
    "print('Sample mean =',s_mean,'+-',tvl * s_se)\n",
    "\n",
    "#create a function that summarize the mean cell values for each cart model within our rndf model for just the cell within our national forest boundary \n",
    "def get_m_mean(ply,emdl,pred_rs):\n",
    "    '''\n",
    "    used to calculate mean and standard error for a geographic subset using an ensemble \n",
    "\n",
    "    ply = (GeoDataFrame or GeoSeries) the area used to subset the image\n",
    "    emdl = (ensemble) the ensemble of models\n",
    "    pred_rs = (raster) the predictor raster that has new input values used in the model.\n",
    "\n",
    "    returns mean and standard error estimates from an ensemble\n",
    "    '''\n",
    "    #clip our predictor raster\n",
    "    cl_pred=clipping.clip(ply,pred_rs)\n",
    "    #create a empty list to store values\n",
    "    lst_vls=[]\n",
    "    #iterate through each CART model and estimate the mean cell value for cells within the forest service boundary\n",
    "    for mdl in emdl.estimators_:\n",
    "        rs=cl_pred.model_predict(mdl)**(1/3)\n",
    "        lst_vls.append(rs.mean().compute())\n",
    "\n",
    "    #return the mean estimate of each models mean and the standard deviation of the mean estimates (se)\n",
    "    return (np.mean(lst_vls),np.std(lst_vls))\n",
    "\n",
    "m_mean,m_se = get_m_mean(nfp,rndf,pred_rs)\n",
    "tvl=scipy.stats.t.ppf(0.975,rndf.n_estimators-1)\n",
    "print('Model se =', m_se)\n",
    "print('Model mean =', m_mean,'+-',tvl * m_se)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9: Applying our models to 2021 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10: Making a dashboard\n",
    "#### In this section we will explore how to convert your results into a functioning dashboard\n",
    "\n",
    "#### Key learning points:\n",
    "- Displaying results\n",
    "- Guiding the user\n",
    "- Hosting"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rstools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
